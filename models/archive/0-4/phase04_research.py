#!/usr/bin/env python3
"""
Phase 0.4 Research Script - Biologically Enhanced Implementation (FIXED)
keywords: [snn, three-factor-learning, dopamine, e/i-balance, refractory, dale-principle, fixed]

This version includes Phase 1 critical fixes:
1. Fixed eligibility trace computation with proper pre/post synaptic tracking
2. Biologically correct Dale's principle implementation
3. Reward prediction error for dopamine signaling

Original features maintained:
- E/I balance (80/20 rule) with Dale's principle
- Three-factor learning with eligibility traces and dopamine modulation
- Refractory periods for biologically plausible spike generation
- Synaptic current dynamics
- Maximum performance optimization (10-100x speedup)
- Complete data export including biological properties
- Progress reporting with ETA
- Command-line interface
"""

import argparse
import gzip
import json
import os
import pickle
import sys
import time
from datetime import datetime
from typing import Dict, NamedTuple, Tuple

import jax
import jax.numpy as jnp
import numpy as np
from jax import jit, random

# Configuration
GRID_WORLD_SIZE = 100
NUM_REWARDS = 15
MAX_EPISODE_STEPS = 50000

# Proximity rewards
PROXIMITY_THRESHOLD_HIGH = 0.8
PROXIMITY_THRESHOLD_MED = 0.6
PROXIMITY_REWARD_HIGH = 0.5
PROXIMITY_REWARD_MED = 0.2


class NetworkParams(NamedTuple):
    """Parameters for the spiking neural network with biological features"""

    NUM_NEURONS: int = 256
    NUM_INPUTS: int = 1
    NUM_OUTPUTS: int = 4

    # E/I balance (80/20 rule)
    EXCITATORY_RATIO: float = 0.8

    # Neuron parameters
    TAU_V: float = 20.0
    V_REST: float = -65.0
    V_THRESHOLD: float = -50.0
    V_RESET: float = -65.0
    REFRACTORY_TIME: float = 2.0  # ms

    # Synapse parameters
    TAU_FAST: float = 20.0
    TAU_SLOW: float = 100.0
    TAU_SYN: float = 10.0  # Synaptic current decay

    # Three-factor learning parameters
    TAU_ELIGIBILITY: float = 1000.0  # Eligibility trace decay (1 second)
    TAU_DOPAMINE: float = 200.0  # Dopamine/neuromodulator decay
    BASELINE_DOPAMINE: float = 0.2

    # Learning parameters
    LEARNING_RATE: float = 0.01
    STDP_WINDOW: float = 20.0
    A_PLUS: float = 0.01
    A_MINUS: float = 0.01

    # PHASE 1 FIX: Reward prediction parameters
    REWARD_PREDICTION_RATE: float = 0.01  # Learning rate for value estimation
    REWARD_DISCOUNT: float = 0.95  # Temporal discount factor


class NetworkState(NamedTuple):
    """State of the spiking neural network with biological features"""

    # Basic neural state
    v: jnp.ndarray  # Membrane potentials
    spike: jnp.ndarray  # Current spikes
    refractory: jnp.ndarray  # Refractory counters

    # E/I identity
    is_excitatory: jnp.ndarray  # True for excitatory neurons

    # Synaptic state
    syn_current: jnp.ndarray  # Synaptic currents
    trace_fast: jnp.ndarray  # Fast synaptic trace
    trace_slow: jnp.ndarray  # Slow synaptic trace

    # Three-factor learning state
    eligibility_trace: jnp.ndarray  # Eligibility traces
    dopamine: float  # Global neuromodulator level

    # Weights
    w: jnp.ndarray  # Recurrent weights
    w_in: jnp.ndarray  # Input weights
    w_out: jnp.ndarray  # Output weights

    # Motor output
    motor_spikes: jnp.ndarray  # Motor neuron spikes
    motor_trace: jnp.ndarray  # Motor neuron traces

    # PHASE 1 FIX: Connection tracking for proper STDP
    connection_mask: jnp.ndarray  # Boolean mask of existing connections

    # PHASE 1 FIX: Reward prediction for dopamine RPE
    value_estimate: float  # Expected future reward
    prev_value: float  # Previous value estimate (for TD error)


# JIT-compiled functions for maximum performance
@jit
def compute_gradient_vectorized(pos, reward_positions, active_rewards, grid_size):
    """Vectorized gradient computation - 100x faster than loops"""

    # Handle empty reward case using JAX conditional
    empty_case = reward_positions.size == 0
    no_active_case = jnp.sum(active_rewards) == 0

    # Early return for edge cases
    def compute_normal_gradient():
        # Ensure reward_positions is properly shaped as 2D
        reward_positions_2d = jnp.reshape(reward_positions, (-1, 2))

        # Toroidal distance calculation - vectorized
        dx = jnp.minimum(
            jnp.abs(pos[0] - reward_positions_2d[:, 0]),
            grid_size - jnp.abs(pos[0] - reward_positions_2d[:, 0]),
        )
        dy = jnp.minimum(
            jnp.abs(pos[1] - reward_positions_2d[:, 1]),
            grid_size - jnp.abs(pos[1] - reward_positions_2d[:, 1]),
        )

        # All distances at once
        distances = jnp.sqrt(dx**2 + dy**2)

        # Only consider active rewards
        distances = jnp.where(active_rewards, distances, 1e6)

        # Find closest distance
        closest_dist = jnp.min(distances)

        # Multi-scale gradient
        gradient = jnp.where(
            closest_dist < 5,
            1.0 - (closest_dist / 5),
            jnp.where(
                closest_dist < 15,
                0.8 - 0.6 * ((closest_dist - 5) / 10),
                0.2 * jnp.exp(-closest_dist / 30),
            ),
        )

        return jnp.where(closest_dist > 1e5, 0.0, gradient)

    # Use JAX conditional instead of Python if
    return jnp.where(empty_case | no_active_case, 0.0, compute_normal_gradient())


@jit
def step_vectorized(agent_pos, action, reward_positions, active_rewards, grid_size):
    """Vectorized step function"""
    moves = jnp.array([[0, 1], [0, -1], [1, 0], [-1, 0]])
    new_pos = (agent_pos + moves[action]) % grid_size

    # Handle empty reward case using JAX conditional
    empty_case = reward_positions.size == 0

    def compute_normal_step():
        # Ensure reward_positions is properly shaped as 2D
        reward_positions_2d = jnp.reshape(reward_positions, (-1, 2))

        # Check for reward collection
        at_reward = jnp.all(reward_positions_2d == new_pos[None, :], axis=1)
        collected_reward = jnp.any(at_reward & active_rewards)

        # Update active rewards
        new_active_rewards = jnp.where(at_reward, False, active_rewards)

        return collected_reward, new_active_rewards

    def empty_case_result():
        return False, active_rewards

    # Use JAX conditional
    collected_reward, new_active_rewards = jax.lax.cond(
        empty_case, lambda: empty_case_result(), lambda: compute_normal_step()
    )

    return new_pos, collected_reward, new_active_rewards


class OptimizedGridWorld:
    """High-performance grid world"""

    def __init__(self, seed: int = 0):
        self.seed = seed
        self.grid_size = GRID_WORLD_SIZE
        self.reset()

    def reset(self):
        """Reset with vectorized reward setup"""
        key = random.PRNGKey(self.seed)
        agent_key, reward_key = random.split(key)

        # Random agent start
        self.agent_pos = random.randint(agent_key, (2,), 0, self.grid_size)

        # Pre-compute all reward positions
        self.reward_positions = self._create_vectorized_rewards(reward_key)
        self.active_rewards = jnp.ones(len(self.reward_positions), dtype=bool)

        self.rewards_collected = 0
        self.proximity_rewards_collected = 0
        self.steps = 0
        self.max_steps = MAX_EPISODE_STEPS

    def _create_vectorized_rewards(self, key):
        """Create clustered rewards"""
        # Adjust clustering parameters for smaller grids
        if self.grid_size <= 20:
            # Fewer clusters for small grids
            num_clusters = max(1, NUM_REWARDS // 3)
            cluster_spread = max(2.0, self.grid_size / 10)  # Smaller spread
        else:
            num_clusters = 20
            cluster_spread = 10.0

        rewards_per_cluster = max(1, NUM_REWARDS // num_clusters)

        all_positions = []
        cluster_keys = random.split(key, num_clusters)

        for cluster_key in cluster_keys:
            center_key, spread_key = random.split(cluster_key)
            center = random.randint(center_key, (2,), 0, self.grid_size)

            # Generate rewards for this cluster
            num_rewards_this_cluster = min(rewards_per_cluster, NUM_REWARDS - len(all_positions))
            if num_rewards_this_cluster <= 0:
                break

            reward_keys = random.split(spread_key, num_rewards_this_cluster)
            for rkey in reward_keys:
                offset = random.normal(rkey, (2,)) * cluster_spread
                pos = (center + offset).astype(jnp.int32) % self.grid_size
                all_positions.append(pos)

                if len(all_positions) >= NUM_REWARDS:
                    break

            if len(all_positions) >= NUM_REWARDS:
                break

        # Ensure we have at least some rewards
        if len(all_positions) == 0:
            # Fallback: place rewards randomly
            fallback_key = random.split(key, NUM_REWARDS)
            for fkey in fallback_key:
                pos = random.randint(fkey, (2,), 0, self.grid_size)
                all_positions.append(pos)

        # Convert to proper 2D array
        positions_array = jnp.array(all_positions[:NUM_REWARDS])

        # Ensure it's always 2D with shape (N, 2)
        if positions_array.ndim == 1:
            positions_array = positions_array.reshape(1, -1)

        return positions_array

    def get_observation(self):
        """Get gradient observation"""
        return compute_gradient_vectorized(
            self.agent_pos, self.reward_positions, self.active_rewards, self.grid_size
        )

    def step(self, action: int):
        """Step environment"""
        prev_gradient = self.get_observation()

        # Optimized step
        new_pos, collected_reward, new_active_rewards = step_vectorized(
            self.agent_pos, action, self.reward_positions, self.active_rewards, self.grid_size
        )

        self.agent_pos = new_pos
        self.active_rewards = new_active_rewards

        if collected_reward:
            self.rewards_collected += 1

        # Get new gradient
        new_gradient = self.get_observation()

        # Proximity rewards
        proximity_reward = 0.0
        if new_gradient > PROXIMITY_THRESHOLD_HIGH and prev_gradient <= PROXIMITY_THRESHOLD_HIGH:
            proximity_reward = PROXIMITY_REWARD_HIGH
            self.proximity_rewards_collected += 1
        elif new_gradient > PROXIMITY_THRESHOLD_MED and prev_gradient <= PROXIMITY_THRESHOLD_MED:
            proximity_reward = PROXIMITY_REWARD_MED
            self.proximity_rewards_collected += 1

        self.steps += 1
        total_reward = float(collected_reward) + proximity_reward

        return total_reward


def initialize_network(key: jax.random.PRNGKey, params: NetworkParams) -> NetworkState:
    """Initialize network with E/I distinction and proper connectivity"""
    keys = random.split(key, 6)

    # Assign E/I identity (first 80% are excitatory)
    num_exc = int(params.NUM_NEURONS * params.EXCITATORY_RATIO)
    is_excitatory = jnp.arange(params.NUM_NEURONS) < num_exc

    # Neural state
    v = jnp.ones(params.NUM_NEURONS) * params.V_REST
    spike = jnp.zeros(params.NUM_NEURONS, dtype=bool)
    refractory = jnp.zeros(params.NUM_NEURONS)

    # Synaptic state
    syn_current = jnp.zeros(params.NUM_NEURONS)
    trace_fast = jnp.zeros(params.NUM_NEURONS)
    trace_slow = jnp.zeros(params.NUM_NEURONS)

    # Initialize weights with E/I constraints
    w_in = random.uniform(keys[0], (params.NUM_INPUTS, params.NUM_NEURONS), minval=0.5, maxval=2.0)

    # PHASE 1 FIX: Create connection mask with biologically plausible probabilities
    # Different connection probabilities by type
    P_EE = 0.1  # Eâ†’E connection probability
    P_EI = 0.3  # Eâ†’I connection probability
    P_IE = 0.4  # Iâ†’E connection probability
    P_II = 0.2  # Iâ†’I connection probability

    # Create connection probability matrix
    prob_matrix = jnp.zeros((params.NUM_NEURONS, params.NUM_NEURONS))
    EE_mask = jnp.outer(is_excitatory, is_excitatory)
    EI_mask = jnp.outer(is_excitatory, ~is_excitatory)
    IE_mask = jnp.outer(~is_excitatory, is_excitatory)
    II_mask = jnp.outer(~is_excitatory, ~is_excitatory)

    prob_matrix = jnp.where(EE_mask, P_EE, prob_matrix)
    prob_matrix = jnp.where(EI_mask, P_EI, prob_matrix)
    prob_matrix = jnp.where(IE_mask, P_IE, prob_matrix)
    prob_matrix = jnp.where(II_mask, P_II, prob_matrix)

    # Generate connections based on probabilities
    connection_rand = random.uniform(keys[1], (params.NUM_NEURONS, params.NUM_NEURONS))
    connection_mask = connection_rand < prob_matrix
    connection_mask = connection_mask.at[jnp.diag_indices(params.NUM_NEURONS)].set(
        False
    )  # No self-connections

    # Initialize recurrent weights only where connections exist
    w = random.uniform(keys[2], (params.NUM_NEURONS, params.NUM_NEURONS), minval=0.0, maxval=0.1)
    w = jnp.where(connection_mask, w, 0.0)

    # PHASE 1 FIX: Apply Dale's principle correctly
    # E neurons have positive weights, I neurons have negative weights
    w = apply_dale_principle_correct(w, is_excitatory)

    # Output weights (only from excitatory neurons to motor outputs)
    w_out = random.uniform(
        keys[3], (params.NUM_NEURONS, params.NUM_OUTPUTS), minval=0.0, maxval=0.1
    )
    w_out = jnp.where(is_excitatory[:, None], w_out, 0.0)

    # Learning state
    eligibility_trace = jnp.zeros((params.NUM_NEURONS, params.NUM_NEURONS))
    dopamine = params.BASELINE_DOPAMINE

    # Motor state
    motor_spikes = jnp.zeros(params.NUM_OUTPUTS, dtype=bool)
    motor_trace = jnp.zeros(params.NUM_OUTPUTS)

    # PHASE 1 FIX: Initialize reward prediction
    value_estimate = 0.0
    prev_value = 0.0

    return NetworkState(
        v=v,
        spike=spike,
        refractory=refractory,
        is_excitatory=is_excitatory,
        syn_current=syn_current,
        trace_fast=trace_fast,
        trace_slow=trace_slow,
        eligibility_trace=eligibility_trace,
        dopamine=dopamine,
        w=w,
        w_in=w_in,
        w_out=w_out,
        motor_spikes=motor_spikes,
        motor_trace=motor_trace,
        connection_mask=connection_mask,
        value_estimate=value_estimate,
        prev_value=prev_value,
    )


@jit
def apply_dale_principle_correct(w: jnp.ndarray, is_excitatory: jnp.ndarray) -> jnp.ndarray:
    """PHASE 1 FIX: Apply Dale's principle with biological accuracy

    Dale's principle: A neuron releases the same neurotransmitter at all its terminals.
    - Excitatory neurons (E) always have positive effects
    - Inhibitory neurons (I) always have negative effects
    """
    # Create masks for neuron types (pre-synaptic = rows)
    E_pre = is_excitatory[:, None]  # Shape: (N, 1)
    I_pre = ~is_excitatory[:, None]  # Shape: (N, 1)

    # Apply sign constraints based on pre-synaptic neuron type
    # E neurons: all outgoing weights positive
    # I neurons: all outgoing weights negative
    w_corrected = jnp.where(E_pre, jnp.abs(w), -jnp.abs(w))

    return w_corrected


@jit
def neuron_step(
    state: NetworkState, input_current: float, params: NetworkParams, key: jax.random.PRNGKey
) -> NetworkState:
    """Neuron step with refractory periods and synaptic currents"""
    BASELINE_CURRENT = 5.0
    NOISE_SCALE = 2.0

    # Update refractory counters
    refractory_new = jnp.maximum(0, state.refractory - 1.0)

    # Synaptic currents with proper decay
    syn_current_new = state.syn_current * jnp.exp(-1.0 / params.TAU_SYN)

    # Add incoming spikes to synaptic current (respecting Dale's principle)
    # Use biologically plausible synaptic strengths
    spike_input = state.spike.astype(float)
    syn_input = jnp.dot(state.w, spike_input)
    syn_current_new = syn_current_new + syn_input

    # External input
    i_ext = state.w_in[0] * input_current * 10.0 + BASELINE_CURRENT
    i_total = i_ext + syn_current_new

    # Add noise
    noise = random.normal(key, state.v.shape) * NOISE_SCALE
    i_total = i_total + noise

    # Membrane potential dynamics
    dv = (-state.v + params.V_REST + i_total) / params.TAU_V
    v_new = state.v + dv

    # Spike generation with refractory period
    can_spike = refractory_new == 0
    spike_new = (v_new >= params.V_THRESHOLD) & can_spike

    # Reset voltage and set refractory period for spiking neurons
    v_new = jnp.where(spike_new, params.V_RESET, v_new)
    refractory_new = jnp.where(spike_new, params.REFRACTORY_TIME, refractory_new)

    # Update traces
    trace_fast_new = state.trace_fast * jnp.exp(-1.0 / params.TAU_FAST) + spike_new * 1.0
    trace_slow_new = state.trace_slow * jnp.exp(-1.0 / params.TAU_SLOW) + spike_new * 0.5

    return state._replace(
        v=v_new,
        spike=spike_new,
        refractory=refractory_new,
        syn_current=syn_current_new,
        trace_fast=trace_fast_new,
        trace_slow=trace_slow_new,
    )


@jit
def compute_eligibility_trace_fixed(state: NetworkState, params: NetworkParams) -> jnp.ndarray:
    """PHASE 1 FIX: Compute eligibility traces with proper pre/post synaptic pairs

    Only computes STDP for existing connections using the connection mask.
    """
    # Get pre and post synaptic activities
    # For each synapse at position (i,j), i is pre-synaptic, j is post-synaptic

    # Expand traces and spikes for all possible pairs
    # pre_trace[i] affects synapse (i,j) when post_spike[j] occurs
    pre_trace_expanded = state.trace_fast[:, None]  # Shape: (N, 1)
    post_spike_expanded = state.spike[None, :]  # Shape: (1, N)

    # post_trace[j] affects synapse (i,j) when pre_spike[i] occurs
    pre_spike_expanded = state.spike[:, None]  # Shape: (N, 1)
    post_trace_expanded = state.trace_fast[None, :]  # Shape: (1, N)

    # STDP computation
    # Pre before post -> LTP
    ltp = pre_trace_expanded * post_spike_expanded.astype(float)
    ltd = pre_spike_expanded.astype(float) * post_trace_expanded  # Post before pre -> LTD

    # Apply STDP only to existing connections
    stdp = jnp.where(state.connection_mask, params.A_PLUS * ltp - params.A_MINUS * ltd, 0.0)

    # Decay eligibility traces
    decay = jnp.exp(-1.0 / params.TAU_ELIGIBILITY)
    new_eligibility = state.eligibility_trace * decay + stdp

    return new_eligibility


@jit
def three_factor_update_fixed(
    state: NetworkState,
    dopamine_modulation: float,  # This is now the RPE-based modulation
    params: NetworkParams,
) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """PHASE 1 FIX: Three-factor learning with proper Dale's principle maintenance"""

    # Weight update proportional to eligibility * dopamine modulation
    dw = params.LEARNING_RATE * dopamine_modulation * state.eligibility_trace

    # Only update existing connections
    dw = jnp.where(state.connection_mask, dw, 0.0)

    # Update weights
    updated_weights = state.w + dw

    # Maintain Dale's principle after update
    updated_weights = apply_dale_principle_correct(updated_weights, state.is_excitatory)

    # Ensure weights stay in reasonable bounds
    # E synapses: [0, 1], I synapses: [-1, 0]
    E_mask = state.is_excitatory[:, None]
    updated_weights = jnp.where(
        E_mask, jnp.clip(updated_weights, 0.0, 1.0), jnp.clip(updated_weights, -1.0, 0.0)
    )

    # Zero out non-existent connections
    updated_weights = jnp.where(state.connection_mask, updated_weights, 0.0)

    # Partially decay eligibility traces after update
    new_eligibility = state.eligibility_trace * 0.9

    return updated_weights, new_eligibility


@jit
def update_dopamine_with_rpe(
    state: NetworkState, reward: float, params: NetworkParams
) -> Tuple[float, float, float]:
    """PHASE 1 FIX: Update dopamine based on reward prediction error

    Returns: (new_dopamine, new_value_estimate, td_error)
    """
    # Compute TD error (reward prediction error)
    # TD error = reward + Î³ * V(s') - V(s)
    td_error = reward + params.REWARD_DISCOUNT * state.value_estimate - state.prev_value

    # Update value estimate using TD learning
    new_value = state.value_estimate + params.REWARD_PREDICTION_RATE * td_error

    # Decay dopamine towards baseline
    decay = jnp.exp(-1.0 / params.TAU_DOPAMINE)
    baseline_dopamine = state.dopamine * decay + params.BASELINE_DOPAMINE * (1 - decay)

    # Dopamine responds to prediction error, not raw reward
    # Positive RPE -> phasic burst, Negative RPE -> phasic dip
    dopamine_response = baseline_dopamine + td_error * 0.5
    new_dopamine = jnp.clip(dopamine_response, 0.0, 2.0)

    return new_dopamine, new_value, td_error


@jit
def compute_dopamine_modulation(dopamine: float, baseline: float) -> float:
    """PHASE 1 FIX: Convert dopamine level to learning modulation factor

    Uses a sigmoid function to create graded learning:
    - Low dopamine (< baseline): LTD dominates (negative modulation)
    - Baseline dopamine: Balanced (near zero modulation)
    - High dopamine (> baseline): LTP dominates (positive modulation)
    """
    # Normalize dopamine relative to baseline
    da_normalized = (dopamine - baseline) / (baseline + 1e-8)

    # Sigmoid modulation: creates smooth transition from LTD to LTP
    # Range approximately [-1, 1]
    modulation = jax.nn.tanh(da_normalized * 2.0)

    return modulation


@jit
def make_decision(state: NetworkState, key: jax.random.PRNGKey) -> Tuple[int, NetworkState]:
    """Make action decision"""
    motor_input = jnp.dot(state.spike.astype(float), state.w_out)
    noise = random.normal(key, motor_input.shape) * 0.5
    motor_input = motor_input + noise

    action_probs = jax.nn.softmax(motor_input * 2.0)
    action = random.categorical(key, jnp.log(action_probs))

    motor_spikes = jnp.zeros(4, dtype=bool).at[action].set(True)
    motor_trace = state.motor_trace * 0.9 + motor_spikes * 1.0

    return action, state._replace(motor_spikes=motor_spikes, motor_trace=motor_trace)


class StreamingDataLogger:
    """
    Stream data to disk during episode to avoid memory limitations
    """

    def __init__(self, episode_dir: str):
        self.episode_dir = episode_dir
        self.files = {}

    def initialize_streams(self, max_steps: int, num_neurons: int, num_rewards: int):
        """Create memory-mapped files for streaming data"""
        # Agent trajectory (already complete in original)

        # Reward states over time
        self.files["reward_states"] = np.memmap(
            f"{self.episode_dir}/reward_states.dat",
            dtype="bool",
            mode="w+",
            shape=(max_steps, num_rewards),
        )

        # Full neural activity (if requested)
        self.files["spikes"] = np.memmap(
            f"{self.episode_dir}/spike_trains_full.dat",
            dtype="bool",
            mode="w+",
            shape=(max_steps, num_neurons),
        )

        self.files["voltages"] = np.memmap(
            f"{self.episode_dir}/voltages_full.dat",
            dtype="float32",
            mode="w+",
            shape=(max_steps, num_neurons),
        )

        # Synaptic weight changes log
        self.weight_change_log = open(f"{self.episode_dir}/weight_changes.csv", "w")
        self.weight_change_log.write("step,pre_idx,post_idx,old_weight,new_weight,delta\n")

    def log_timestep(self, step: int, state, env):
        """Log data for current timestep"""
        # Reward states
        self.files["reward_states"][step] = env.active_rewards

        # Neural activity
        self.files["spikes"][step] = state.spike
        self.files["voltages"][step] = state.v

    def log_weight_change(self, step: int, pre_idx: int, post_idx: int, old_w: float, new_w: float):
        """Log individual synaptic weight change"""
        delta = new_w - old_w
        self.weight_change_log.write(
            f"{step},{pre_idx},{post_idx},{old_w:.6f},{new_w:.6f},{delta:.6f}\n"
        )

    def close(self):
        """Close all file handles"""
        for f in self.files.values():
            if hasattr(f, "flush"):
                f.flush()
        self.weight_change_log.close()


def run_episode(seed: int, progress_callback=None, full_trace=False, episode_dir=None) -> Dict:
    """Run single episode with comprehensive data collection

    Args:
        seed: Random seed for reproducibility
        progress_callback: Optional callback for progress updates
        full_trace: If True, save complete neural dynamics for every timestep
        episode_dir: Directory to save streaming data (only used if full_trace=True)
    """

    # Initialize
    key = random.PRNGKey(seed)
    network_key, episode_key = random.split(key)

    params = NetworkParams()
    state = initialize_network(network_key, params)
    env = OptimizedGridWorld(seed)

    # Pre-allocate arrays for maximum performance
    max_steps = env.max_steps

    # Initialize streaming logger if full trace requested
    logger = None
    if full_trace and episode_dir:
        logger = StreamingDataLogger(episode_dir)
        logger.initialize_streams(max_steps, params.NUM_NEURONS, NUM_REWARDS)

    # Episode data arrays
    gradients = np.zeros(max_steps, dtype=np.float32)
    positions = np.zeros((max_steps, 2), dtype=np.int32)
    actions = np.zeros(max_steps, dtype=np.int8)
    rewards = np.zeros(max_steps, dtype=np.float32)

    # Neural dynamics arrays (sampled to save memory)
    sample_interval = 100  # Sample every 100 steps
    num_samples = max_steps // sample_interval
    spike_trains = np.zeros((num_samples, params.NUM_NEURONS), dtype=bool)
    membrane_potentials = np.zeros((num_samples, params.NUM_NEURONS), dtype=np.float32)
    synaptic_traces_fast = np.zeros((num_samples, params.NUM_NEURONS), dtype=np.float32)
    synaptic_traces_slow = np.zeros((num_samples, params.NUM_NEURONS), dtype=np.float32)
    synaptic_currents = np.zeros((num_samples, params.NUM_NEURONS), dtype=np.float32)
    refractory_states = np.zeros((num_samples, params.NUM_NEURONS), dtype=np.float32)
    dopamine_levels = np.zeros(num_samples, dtype=np.float32)
    # PHASE 1 FIX: Track TD errors
    td_errors = np.zeros(num_samples, dtype=np.float32)

    # Track reward collection
    reward_steps = []
    proximity_reward_steps = []
    unique_positions = set()

    # Weight snapshots (save periodically)
    weight_snapshot_interval = 5000
    weight_snapshots = []

    episode_start_time = time.time()

    # Episode loop
    for step in range(max_steps):
        # Get observation
        gradient = env.get_observation()
        gradients[step] = gradient
        positions[step] = env.agent_pos

        # Track unique positions periodically
        if step % 100 == 0:
            unique_positions.add(tuple(int(x) for x in env.agent_pos))

        # Neural dynamics step
        episode_key, neuron_key = random.split(episode_key)
        state = neuron_step(state, gradient, params, neuron_key)

        # PHASE 1 FIX: Compute eligibility traces with proper pre/post tracking
        new_eligibility = compute_eligibility_trace_fixed(state, params)
        state = state._replace(eligibility_trace=new_eligibility)

        # Make decision
        episode_key, decision_key = random.split(episode_key)
        action, state = make_decision(state, decision_key)
        actions[step] = action

        # Step environment
        reward = env.step(action)
        rewards[step] = reward

        # PHASE 1 FIX: Update value estimate for next step
        state = state._replace(prev_value=state.value_estimate)

        # PHASE 1 FIX: Update dopamine based on reward prediction error
        new_dopamine, new_value, td_error = update_dopamine_with_rpe(state, reward, params)
        state = state._replace(dopamine=new_dopamine, value_estimate=new_value)

        # PHASE 1 FIX: Compute dopamine modulation factor (graded, not binary)
        dopamine_modulation = compute_dopamine_modulation(state.dopamine, params.BASELINE_DOPAMINE)

        # PHASE 1 FIX: Three-factor learning with graded modulation
        # Always allow learning, but direction/magnitude depends on dopamine
        new_w, new_eligibility = three_factor_update_fixed(
            state._replace(eligibility_trace=new_eligibility), dopamine_modulation, params
        )
        state = state._replace(w=new_w, eligibility_trace=new_eligibility)

        # Track rewards
        if reward >= 1.0:  # Actual reward
            reward_steps.append(step)
        elif reward > 0:  # Proximity reward
            proximity_reward_steps.append(step)

        # Log full trace data if requested
        if logger:
            logger.log_timestep(step, state, env)
            # TODO: Add weight change logging when weights are updated

        # Sample neural dynamics
        if step % sample_interval == 0:
            sample_idx = step // sample_interval
            spike_trains[sample_idx] = np.array(state.spike)
            membrane_potentials[sample_idx] = np.array(state.v)
            synaptic_traces_fast[sample_idx] = np.array(state.trace_fast)
            synaptic_traces_slow[sample_idx] = np.array(state.trace_slow)
            synaptic_currents[sample_idx] = np.array(state.syn_current)
            refractory_states[sample_idx] = np.array(state.refractory)
            dopamine_levels[sample_idx] = float(state.dopamine)
            # PHASE 1 FIX: Save TD error
            td_errors[sample_idx] = float(td_error)

        # Save weight snapshots
        if step % weight_snapshot_interval == 0:
            weight_snapshots.append(
                {
                    "step": step,
                    "w": np.array(state.w),
                    "w_in": np.array(state.w_in),
                    "w_out": np.array(state.w_out),
                    "eligibility": np.array(state.eligibility_trace),
                    "dopamine": float(state.dopamine),
                    "value_estimate": float(state.value_estimate),  # PHASE 1 FIX
                    "td_error": float(td_error),  # PHASE 1 FIX
                }
            )

        # Progress callback
        if progress_callback and step % 1000 == 0:
            progress_callback(step, max_steps, len(reward_steps), len(proximity_reward_steps))

    # Final weight snapshot
    weight_snapshots.append(
        {
            "step": max_steps,
            "w": np.array(state.w),
            "w_in": np.array(state.w_in),
            "w_out": np.array(state.w_out),
            "eligibility": np.array(state.eligibility_trace),
            "dopamine": float(state.dopamine),
            "value_estimate": float(state.value_estimate),
            "td_error": 0.0,  # No error at end
        }
    )

    # Close streaming logger if used
    if logger:
        logger.close()

    episode_time = time.time() - episode_start_time

    # Comprehensive results
    return {
        "metadata": {
            "seed": seed,
            "episode_time_seconds": episode_time,
            "steps_completed": max_steps,
            "rewards_collected": len(reward_steps),
            "proximity_rewards_collected": len(proximity_reward_steps),
            "unique_positions_visited": len(unique_positions),
            "network_params": params._asdict(),
        },
        "trajectory": {
            "positions": positions,
            "actions": actions,
            "gradients": gradients,
            "rewards": rewards,
            "reward_steps": np.array(reward_steps),
            "proximity_reward_steps": np.array(proximity_reward_steps),
        },
        "neural_dynamics": {
            "spike_trains": spike_trains,
            "membrane_potentials": membrane_potentials,
            "synaptic_traces_fast": synaptic_traces_fast,
            "synaptic_traces_slow": synaptic_traces_slow,
            "synaptic_currents": synaptic_currents,
            "refractory_states": refractory_states,
            "dopamine_levels": dopamine_levels,
            "td_errors": td_errors,  # PHASE 1 FIX: Include TD errors
            "sample_interval": sample_interval,
        },
        "network_properties": {
            "is_excitatory": np.array(state.is_excitatory),
            "num_excitatory": int(np.sum(state.is_excitatory)),
            "num_inhibitory": int(np.sum(~state.is_excitatory)),
            "connection_mask": np.array(state.connection_mask),  # PHASE 1 FIX
        },
        "weight_evolution": weight_snapshots,
        "environment": {
            "grid_size": GRID_WORLD_SIZE,
            "num_rewards": NUM_REWARDS,
            "reward_positions": np.array(env.reward_positions),
            "initial_agent_pos": np.array(positions[0]),
        },
    }


def reconstruct_reward_field(episode_dir: str, timestep: int):
    """
    Reconstruct exact reward field at any timestep
    """
    reward_states = np.memmap(f"{episode_dir}/reward_states.dat", dtype="bool", mode="r")
    env_data = np.load(f"{episode_dir}/environment.npz")

    active_rewards = reward_states[timestep]
    reward_positions = env_data["reward_positions"][active_rewards]

    return reward_positions


def trace_spike_causality(episode_dir: str, neuron_idx: int, start_step: int, window: int = 100):
    """
    Trace what caused a specific neuron to spike
    """
    spikes = np.memmap(f"{episode_dir}/spike_trains_full.dat", dtype="bool", mode="r")
    voltages = np.memmap(f"{episode_dir}/voltages_full.dat", dtype="float32", mode="r")

    # Find spike time
    spike_times = np.where(spikes[start_step : start_step + window, neuron_idx])[0]

    if len(spike_times) == 0:
        return None

    spike_time = start_step + spike_times[0]

    # Look at pre-synaptic activity
    network_props = np.load(f"{episode_dir}/network_properties.npz")
    connection_mask = network_props["connection_mask"]

    # Which neurons connect to this neuron?
    pre_synaptic = np.where(connection_mask[:, neuron_idx])[0]

    # Check their recent activity
    recent_window = 20  # ms
    pre_spike_times = {}
    for pre_idx in pre_synaptic:
        pre_spikes = np.where(spikes[spike_time - recent_window : spike_time, pre_idx])[0]
        if len(pre_spikes) > 0:
            pre_spike_times[pre_idx] = spike_time - recent_window + pre_spikes

    return {
        "neuron": neuron_idx,
        "spike_time": spike_time,
        "voltage_trajectory": voltages[spike_time - 50 : spike_time + 10, neuron_idx],
        "pre_synaptic_spikes": pre_spike_times,
    }


def save_episode_data(episode_data: Dict, output_dir: str, episode_num: int):
    """Save episode data to compressed files"""
    episode_dir = os.path.join(output_dir, f"episode_{episode_num:03d}")
    os.makedirs(episode_dir, exist_ok=True)

    # Save metadata as JSON
    with open(os.path.join(episode_dir, "metadata.json"), "w") as f:
        json.dump(episode_data["metadata"], f, indent=2)

    # Save trajectory data
    np.savez_compressed(os.path.join(episode_dir, "trajectory.npz"), **episode_data["trajectory"])

    # Save neural dynamics
    np.savez_compressed(
        os.path.join(episode_dir, "neural_dynamics.npz"), **episode_data["neural_dynamics"]
    )

    # Save weight evolution
    with gzip.open(os.path.join(episode_dir, "weight_evolution.pkl.gz"), "wb") as f:
        pickle.dump(episode_data["weight_evolution"], f)

    # Save environment info
    np.savez_compressed(os.path.join(episode_dir, "environment.npz"), **episode_data["environment"])

    # Save network properties
    np.savez_compressed(
        os.path.join(episode_dir, "network_properties.npz"), **episode_data["network_properties"]
    )

    # Create summary CSV for quick access
    summary_data = {
        "episode": episode_num,
        "seed": episode_data["metadata"]["seed"],
        "time_seconds": episode_data["metadata"]["episode_time_seconds"],
        "rewards": episode_data["metadata"]["rewards_collected"],
        "proximity_rewards": episode_data["metadata"]["proximity_rewards_collected"],
        "coverage": episode_data["metadata"]["unique_positions_visited"],
    }

    return summary_data


def run_experiment(args):
    """Run complete experiment with specified parameters"""

    # Create output directory
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(args.output_dir, timestamp)
    os.makedirs(output_dir, exist_ok=True)

    # Save configuration
    config = {
        "num_episodes": args.num_episodes,
        "seeds": list(range(args.seed_start, args.seed_start + args.num_episodes)),
        "grid_size": GRID_WORLD_SIZE,
        "num_rewards": NUM_REWARDS,
        "max_steps": MAX_EPISODE_STEPS,
        "network_params": NetworkParams()._asdict(),
        "proximity_thresholds": {"high": PROXIMITY_THRESHOLD_HIGH, "med": PROXIMITY_THRESHOLD_MED},
        "proximity_rewards": {"high": PROXIMITY_REWARD_HIGH, "med": PROXIMITY_REWARD_MED},
        "timestamp": timestamp,
        "command": " ".join(sys.argv),
        "full_trace": args.full_trace,
        "phase1_fixes": {  # PHASE 1 FIX: Document fixes
            "eligibility_trace": "Fixed with proper pre/post synaptic tracking",
            "dale_principle": "Correctly applied based on pre-synaptic type",
            "dopamine": "Reward prediction error instead of raw reward",
            "learning": "Graded modulation instead of binary threshold",
        },
    }

    with open(os.path.join(output_dir, "config.json"), "w") as f:
        json.dump(config, f, indent=2)

    print("\nðŸ§ª Phase 0.4 Research Experiment (FIXED)")
    print(f"  Episodes: {args.num_episodes}")
    print(f"  Seeds: {args.seed_start} to {args.seed_start + args.num_episodes - 1}")
    print(f"  Output: {output_dir}")
    print(f"  Progress: {'ON' if args.progress else 'OFF'}")
    print(
        f"  Data Export: {'FULL TRACE (memory intensive)' if args.full_trace else 'COMPREHENSIVE'}"
    )
    print("\n  Phase 1 Fixes Applied:")
    print("    âœ“ Eligibility traces with proper synapse tracking")
    print("    âœ“ Biologically correct Dale's principle")
    print("    âœ“ Reward prediction error for dopamine")
    print("    âœ“ Graded dopamine-dependent learning")

    # Estimate total experiment time
    estimated_time_per_episode = 25.0  # seconds, based on test runs
    estimated_total_time = args.num_episodes * estimated_time_per_episode

    def format_time(seconds):
        if seconds < 60:
            return f"{seconds:.0f}s"
        elif seconds < 3600:
            return f"{seconds // 60:.0f}m{seconds % 60:.0f}s"
        else:
            hours = seconds // 3600
            minutes = (seconds % 3600) // 60
            return f"{hours:.0f}h{minutes:.0f}m"

    print(f"  Estimated Time: {format_time(estimated_total_time)}")
    print()

    experiment_start = time.time()
    summaries = []

    def progress_callback(step, max_steps, rewards, proximity):
        """Progress reporting callback"""
        if not args.progress:
            return

        progress_pct = (step / max_steps) * 100
        elapsed = time.time() - episode_start
        steps_per_sec = step / elapsed if elapsed > 0 else 0
        eta = (max_steps - step) / steps_per_sec if steps_per_sec > 0 else 0

        # Progress bar
        bar_width = 30
        filled = int(bar_width * progress_pct / 100)
        bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)

        # Update line
        line = (
            f"\r  {bar} {progress_pct:5.1f}% | "
            f"{step:,}/{max_steps:,} | "
            f"{steps_per_sec:.0f} steps/s | "
            f"ETA: {eta:.0f}s | "
            f"R: {rewards} | P: {proximity}"
        )

        print(line, end="", flush=True)

    # Run episodes
    for i in range(args.num_episodes):
        seed = args.seed_start + i

        print(f"\nðŸ“Š Episode {i + 1}/{args.num_episodes} (Seed {seed})")
        episode_start = time.time()

        # Create episode directory if full trace
        episode_dir = None
        if args.full_trace:
            episode_dir = os.path.join(output_dir, f"episode_{i:03d}")
            os.makedirs(episode_dir, exist_ok=True)

        # Run episode with progress
        episode_data = run_episode(
            seed,
            progress_callback if args.progress else None,
            full_trace=args.full_trace,
            episode_dir=episode_dir,
        )

        if args.progress:
            print()  # New line after progress

        # Save episode data
        summary = save_episode_data(episode_data, output_dir, i)
        summaries.append(summary)

        # Episode summary
        print(
            f"  âœ… Complete: {summary['rewards']} rewards, "
            f"{summary['proximity_rewards']} proximity, "
            f"{summary['coverage']} positions, "
            f"{summary['time_seconds']:.1f}s"
        )

        # Experiment progress
        if i < args.num_episodes - 1:
            total_elapsed = time.time() - experiment_start
            avg_time = total_elapsed / (i + 1)
            eta = avg_time * (args.num_episodes - i - 1)
            print(
                f"  ðŸ“ˆ Experiment: {(i + 1) / args.num_episodes * 100:.0f}% | "
                f"Avg: {avg_time:.1f}s | ETA: {eta:.0f}s"
            )

    # Save experiment summary
    experiment_time = time.time() - experiment_start

    # Calculate statistics
    summary_df = {
        "episodes": args.num_episodes,
        "total_time_seconds": experiment_time,
        "avg_time_per_episode": experiment_time / args.num_episodes,
        "avg_rewards": np.mean([s["rewards"] for s in summaries]),
        "std_rewards": np.std([s["rewards"] for s in summaries]),
        "avg_proximity": np.mean([s["proximity_rewards"] for s in summaries]),
        "std_proximity": np.std([s["proximity_rewards"] for s in summaries]),
        "avg_coverage": np.mean([s["coverage"] for s in summaries]),
        "std_coverage": np.std([s["coverage"] for s in summaries]),
        "episodes_summary": summaries,
    }

    with open(os.path.join(output_dir, "experiment_summary.json"), "w") as f:
        json.dump(summary_df, f, indent=2)

    # Create simple CSV for quick analysis
    import csv

    with open(os.path.join(output_dir, "results.csv"), "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=summaries[0].keys())
        writer.writeheader()
        writer.writerows(summaries)

    print("\nâœ… Experiment Complete!")
    print(f"  Total time: {experiment_time:.1f}s")
    print(f"  Avg rewards: {summary_df['avg_rewards']:.1f} Â± {summary_df['std_rewards']:.1f}")
    print(f"  Avg proximity: {summary_df['avg_proximity']:.1f} Â± {summary_df['std_proximity']:.1f}")
    print(f"  Avg coverage: {summary_df['avg_coverage']:.0f} Â± {summary_df['std_coverage']:.0f}")
    print(f"  Data saved to: {output_dir}")
    print("\nðŸ“ Output Structure:")
    print(f"  {output_dir}/")
    print("    â”œâ”€â”€ config.json              # Full experiment configuration")
    print("    â”œâ”€â”€ experiment_summary.json  # Statistical summary")
    print("    â”œâ”€â”€ results.csv              # Quick-access results table")
    print("    â””â”€â”€ episode_XXX/             # Per-episode data")
    print("        â”œâ”€â”€ metadata.json        # Episode metadata")
    print("        â”œâ”€â”€ trajectory.npz       # Position, action, reward data")
    print("        â”œâ”€â”€ neural_dynamics.npz  # Spike trains, membrane potentials, TD errors")
    print("        â”œâ”€â”€ weight_evolution.pkl.gz # Weight snapshots over time")
    print("        â”œâ”€â”€ environment.npz      # Reward positions, grid info")
    print("        â””â”€â”€ network_properties.npz # E/I identity, connectivity")
    if args.full_trace:
        print("        # Full trace files (when --full-trace is used):")
        print("        â”œâ”€â”€ reward_states.dat    # Active rewards at each timestep")
        print("        â”œâ”€â”€ spike_trains_full.dat # Complete spike trains")
        print("        â”œâ”€â”€ voltages_full.dat    # Complete membrane potentials")
        print("        â””â”€â”€ weight_changes.csv   # Detailed synaptic updates")


def main():
    """Main entry point with argument parsing"""
    parser = argparse.ArgumentParser(
        description="Phase 0.4 Research Script - Optimized SNN Learning (FIXED)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Quick test run
  python phase04_research_fixed.py -n 1

  # Standard research run
  python phase04_research_fixed.py -n 10 --progress

  # Large statistical study
  python phase04_research_fixed.py -n 100 --seed-start 0 --output-dir results/large_study

  # Silent high-performance run
  python phase04_research_fixed.py -n 50 --no-progress
        """,
    )

    parser.add_argument(
        "-n", "--num-episodes", type=int, default=3, help="Number of episodes to run (default: 3)"
    )
    parser.add_argument(
        "-s", "--seed-start", type=int, default=0, help="Starting seed value (default: 0)"
    )
    parser.add_argument(
        "-o", "--output-dir", type=str, default="logs", help="Output directory (default: logs)"
    )
    parser.add_argument(
        "--progress", action="store_true", default=True, help="Show progress bars (default: True)"
    )
    parser.add_argument(
        "--no-progress",
        action="store_false",
        dest="progress",
        help="Disable progress bars for maximum performance",
    )
    parser.add_argument(
        "--full-trace",
        action="store_true",
        default=False,
        help="Save complete neural dynamics for every timestep (memory intensive)",
    )

    args = parser.parse_args()

    # Run experiment
    run_experiment(args)


if __name__ == "__main__":
    main()


# STDP Notes
# firing deltaT affects learning rate and LTP/LTD balance

# Log
# Brian2 log spike times
# select / sample population

# Weight visualization over time

# Spiking visualization over time

# Connections visualization over time / only changes after reward anyways
# Change diff matrix
